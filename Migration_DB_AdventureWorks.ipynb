{"cells":[{"cell_type":"markdown","source":["-sandbox\n##Migração Azure SQL -> Databricks -> Delta Lake\n                                                   \n\n\n<div style=\"text-align: center; line-height: 0; padding-top: 10px;\">\n  <img src=\"https://www.tapclicks.com/wp-content/uploads/AzureSQL.svg\" alt=\"Databricks Learning\" style=\"width:250px\">\n\n  <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Databricks_Logo.png/220px-Databricks_Logo.png\" alt=\"Databricks Learning\" style=\"width: 250px\">\n\n > <img src=\"https://docs.delta.io/latest/_static/delta-lake-logo.png\" alt=\"Databricks Learning\" style=\"width: 250px\">\n\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a41b55b1-6650-4809-9b72-72c20ee60a16"}}},{"cell_type":"code","source":["from delta.tables import *\nfrom pyspark.sql.types import *\nfrom pyspark.sql.functions import col\nimport pytz\nfrom pyspark.sql.functions import lit\nfrom pyspark.sql import Window\nfrom datetime import date\nfrom datetime import datetime\n#from datetime import timedelta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Bibliotecas","showTitle":true,"inputWidgets":{},"nuid":"c9526df7-786e-4bd7-974b-15320ab027a9"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Variáveis de location e de database\ndelta_path_svr = \"/mnt/silver/db_migration/\"\ndelta_path_brz = \"/mnt/bronze/db_migration/\"\ndb_svr = 'silver'\ndb_brz = 'bronze'\n# Variáveis para try/except\nNOTEBOOK_RESULT = \"{'result':{'status': {'statusCode': 'status_code', 'message': 'status_message' }}}\"\nmessage = NOTEBOOK_RESULT.replace(\"status_code\",\"1\").replace(\"status_message\",\"SUCCESFUL\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Varíáveis","showTitle":true,"inputWidgets":{},"nuid":"46f471dd-f3f5-494f-b191-c8205b41189a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Variáveis p/ conexão JDBC no Azure SQL. Em algumas delas foram usados secrets de Key Vault p/ boas práticas de segurança\njdbcHostname = \"server-migration.database.windows.net\"\njdbcPort     =  1433\njdbcDatabase = \"AdventureWorks\"\njdbcUsername = dbutils.secrets.get(scope=\"azurescope\", key=\"mig-usr-database\") \njdbcPassword = dbutils.secrets.get(scope=\"azurescope\", key=\"mig-pwd-database\")\njdbcUrl      = \"jdbc:sqlserver://{0}:{1};database={2};user={3};password={4}\".format(jdbcHostname, jdbcPort, jdbcDatabase, jdbcUsername, jdbcPassword)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Variáveis p/ Conexão JDBC","showTitle":true,"inputWidgets":{},"nuid":"61feadba-800c-4390-a7ee-3e5daa9fe408"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["try:\n        \n  # Conexão JDBC\n    jdbcUrl = f\"jdbc:sqlserver://{jdbcHostname}:{jdbcPort};database={jdbcDatabase}\"\n    connectionProperties = {\n                            \"user\"     : jdbcUsername,\n                            \"password\" : jdbcPassword,\n                            \"driver\"   : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n                           }\nexcept Exception as ex:\n    print(f\"ERRO => {ex}\")    \n    message = NOTEBOOK_RESULT.replace(\"status_code\",\"0\").replace(\"status_message\",str(ex).replace(\"'\",\"`\"))  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Conexão JDBC","showTitle":true,"inputWidgets":{},"nuid":"b6228e4a-df8b-4aa1-b603-708ccd61bf5e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["try:\n    \n  # Query que verifica todas as tables e views do db AdventureWorks\n  pushdown_query = \"\"\" \n                       (select \n                            s.name + '.' + t.name Tables\n                           \n                        from sys.tables t \n                         left join sys.schemas s \n                          on t.schema_id = s.schema_id \n                  ------------------------------------------------      \n                                      union all \n                  ------------------------------------------------\n                        select \n                              s.name + '.' + v.name \n                        from sys.views v \n                         left join sys.schemas s \n                          on v.schema_id = s.schema_id) tbls \"\"\"\n\n  # Dataframe que armazena a query\n  df_stg = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\n  \nexcept Exception as ex:\n    print(f\"ERRO => {ex}\")    \n    message = NOTEBOOK_RESULT.replace(\"status_code\",\"0\").replace(\"status_message\",str(ex).replace(\"'\",\"`\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Query System Tables e System Views","showTitle":true,"inputWidgets":{},"nuid":"76658de6-cba8-4913-87b7-3f57fd4e755c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["try:\n    # Variável que coleta todos os dados de todas as tables e views do db\n    lst_tbls = df_stg.collect()\n    \n    # Variável que não trará 3 tabelas na migração    \n    lst_exc = [\"dbo.ErrorLog\",\"dbo.BuildVersion\",\"sys.database_firewall_rules\",\"sys.ipv6_database_firewall_rules\"]\n        #------------------------------------------------------------------------------------------------------------------#\n    \n    # Loop p/ se capturar todas as tabelas e views e não trazer as tabelas que estão atribuídas à variável lst_exc    \n    for i in lst_tbls:\n\n        if i[\"Tables\"] in lst_exc:\n            continue\n        # Nome das entidades\n        entity = i[\"Tables\"].replace('.','_')\n        \n        #------------------------------------------------------------------------------------------------------------------#\n        \n        # Aqui é criada a variável e dataframe que trazem todos os dados de todas as tabelas e views        \n        pushdown_query = \"(select * from {0}) query\".format(i[\"Tables\"])        \n        df_fnl = spark.read.jdbc(url=jdbcUrl, table=pushdown_query, properties=connectionProperties)\n        \n        #------------------------------------------------------------------------------------------------------------------#\n                \n        # Novo campo para controle da data de migração das tabelas\n        df_fnl = df_fnl.withColumn(\"MigrationCreationDate\",lit(str(datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%Y-%m-%d %H:%M:%S.%f'))).cast(\"timestamp\"))\n                \n        # Aqui é feita a persistência da tabela delta dando overwrite nos dados\n        df_fnl.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"false\").save(delta_path_svr + entity)\n        \n        # Drop de tabelas caso as mesmas ja existam no db silver\n        sql_drop = (f\"drop table if exists {db_svr}.{entity}\")\n        print('=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=\\n',sql_drop)\n        spark.sql(sql_drop)\n        \n        # Criação das tabelas de metadados no db silver do Databricks\n        sql_create= (f\" create table {db_svr}.{entity} using delta location '{delta_path_svr}{entity}'\")\n        print(sql_create,'\\n Tabela Criada com Sucesso!')\n        spark.sql(sql_create)\n        \n        #------------------------------------------------------------------------------------------------------------------#\n        \n        # Aqui é feito o cast do tipo de dado de todas as colunas de cada tabela para string\n        # Devido a ingestão ser na camada bronze neste momento\n        # Geralmente na camada bronze trazemos os dados brutos como texto        \n        for i in df_fnl.columns:\n            df_fnl = df_fnl.withColumn(i,col(i).cast(StringType()))\n        \n        #------------------------------------------------------------------------------------------------------------------#\n        \n        # Novo campo para controle da data de migração das tabelas     \n        df_fnl = df_fnl.withColumn(\"MigrationCreationDate\",lit(str(datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%Y-%m-%d %H:%M:%S.%f'))).cast(\"string\"))\n                        \n        # Aqui é feita a persistência da tabela delta dando overwrite nos dados\n        df_fnl.write.format(\"delta\").mode(\"overwrite\").option(\"mergeSchema\", \"false\").save(delta_path_brz + entity)\n                         \n        print(' ')\n        # Drop de tabelas caso as mesmas ja existam no db silver\n        sql_drop = (f\" drop table if exists {db_brz}.{entity}\")\n        print(sql_drop)\n        spark.sql(sql_drop)\n        \n        # Criação das tabelas de metadados no db silver do Databricks\n        sql_create = (f\" create table {db_brz}.{entity} using delta location '{delta_path_brz}{entity}'\")\n        print(sql_create,'\\n Tabela Criada com Sucesso!')\n        spark.sql(sql_create)\n        \n       #------------------------------------------------------------------------------------------------------------------#\n    print('                           =-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=*MIGRAÇÃO CONCLUÍDA COM SUCESSO!*=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=-=')\n    \nexcept Exception as ex:\n    print(f\"ERRO => {ex}\")    \n    message = NOTEBOOK_RESULT.replace(\"status_code\",\"0\").replace(\"status_message\",str(ex).replace(\"'\",\"`\"))    "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Criação de Tabelas Delta e DDL de Tabelas de Metadados","showTitle":true,"inputWidgets":{},"nuid":"ce6d13d5-7a73-413b-86eb-e49d8f69f4d7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["try:\n  \n  df_stg.unpersist()\n  del df_stg\n    \n  df_fnl.unpersist()\n  del df_fnl\n  \nexcept Exception as ex:\n    print(f\"Waring => {ex}\")  \n    pass"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Remoção de todos os blocos de dataframes da memória e do disco","showTitle":true,"inputWidgets":{},"nuid":"e8fbe00a-6b49-4d7c-9724-9efe8317b906"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["dbutils.notebook.exit(message)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Saída do Notebook","showTitle":true,"inputWidgets":{},"nuid":"9753df7d-20bd-44de-8d57-179cd622b747"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Migration_DB_AdventureWorks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1707899380610948}},"nbformat":4,"nbformat_minor":0}
